{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Memoria.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.3.1"
    },
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jSpFqa3dvYl",
        "colab_type": "text"
      },
      "source": [
        "# Proyecto TID, Accidentes de EEUU\n",
        "\n",
        "Componentes del grupo \n",
        "- Lucas Bodson Lobato\n",
        "- Eduardo Díaz Hernández\n",
        "- Esther Jorge Paramio\n",
        "- Victoria Manrique Rolo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7KKlr6QpJoR",
        "colab_type": "text"
      },
      "source": [
        "# Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpAyx9mheXSo",
        "colab_type": "text"
      },
      "source": [
        "El gobierno registra los accidentes de EEUU durante varios años en cada estado. Anotando entre otros la posición geográfica, el estado en el que ocurre, las condiciones meteorológicas, la hora, duración y su severidad.\n",
        "\n",
        "Algunas de las preguntas que podrían plantearse son:\n",
        "- ¿Podemos predecir la severidad del accidente?\n",
        "- ¿De qué depende esta severidad?\n",
        "- ¿Qué tipo de accidentes tenemos?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvdk3zyphIaW",
        "colab_type": "text"
      },
      "source": [
        "```can be used for numerous applications such as real-time car accident prediction, studying car accidents hotspot locations, casualty analysis and extracting cause and effect rules to predict car accidents, and studying the impact of precipitation or other environmental stimuli on accident occurrence```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rln_Pbucf65J",
        "colab_type": "text"
      },
      "source": [
        "# Datos\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JF07HRKhVD4",
        "colab_type": "text"
      },
      "source": [
        "Estos datos han sido recolectados en tiempo real usando múltiples API de tráfico, desde febrero de 2016 hasta diciembre de 2019 para los EEUU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAp8p6aAhXup",
        "colab_type": "text"
      },
      "source": [
        "### Variables disponibles\n",
        "Entre las variables disponibles tenemos las podemos dividir en tres categorías:\n",
        "\n",
        "**Datos geográficos:**\n",
        "Tenemos longitud y latitud, severidad, hora, distancia, dirección incluyendo número, calle, ciudad, provincia y estado y zona horaria.\n",
        "\n",
        "**Datos meteorológicos:**\n",
        "Tenemos temperatura, viento, humedad, presión del aire, visibilidad, dirección y velocidad del viento, precipitaciones, condiciones meteorológicas y presencia/ausencia de luz natural.\n",
        "\n",
        "**Condiciones de la carretera:**\n",
        "Tenemos baches, intersección, ceda el paso, incorporación, calle sin salida, vías de tren, rotonda, estaciones de servicio, stop, aumento/dismunición de tráfico, señales de tráfico y cambio de sentido. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSihM33mkfRS",
        "colab_type": "text"
      },
      "source": [
        "## Preprocesado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp_myjh9kjUx",
        "colab_type": "text"
      },
      "source": [
        "Dado que el dataset original tenía una gran cantidad de datos, hemos optado por reducir su tamaño. En primer lugar eliminaremos aquellos estados que tengan menos de mil datos y de los restantes escogeremos mil datos de forma aleatoria. Esto lo hemos hecho en python utilizando las librerías **pandas** y **numpy**.\n",
        "\n",
        "Para poder trabajar con ellos de una forma más fácil hemos sustituido todos los valores de tipo string (cadena) por un equivalente numérico. Esto nos ayudará a la hora de mostrar gráfica y realizar tareas de clustering y árboles de decisión.\n",
        "\n",
        "Debajo se encuentra el código utilizado para realizar este preprocesamiento.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8cetYs8pqXR",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "from __future__ import division\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from neupy import algorithms, utils, init\n",
        "from datetime import datetime\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "print(\"leyendo datos\\n\")\n",
        "datos = pd.read_csv('US_Accidents_Dec19.csv', encoding = \"ISO-8859-1\")\n",
        "\n",
        "# conseguir datos filtrados\n",
        "filteredData = datos.drop(['ID', 'TMC', 'Start_Time', 'End_Time', 'Distance(mi)', 'City', 'Country', 'End_Lat', 'End_Lng', 'Description', 'Number', 'Street', 'Side', 'Zipcode', 'Airport_Code', 'Weather_Timestamp'], axis=1)\n",
        "filteredData.to_csv('deleted-col-data.csv', index=False)\n",
        "filteredData = pd.read_csv('deleted-col-data.csv', encoding = \"ISO-8859-1\")\n",
        "filteredData = filteredData.dropna()\n",
        "filteredData.to_csv('deleted-col-data.csv', index=False)\n",
        "filteredData = pd.read_csv('deleted-col-data.csv', encoding = \"ISO-8859-1\")\n",
        "#medias por estados\n",
        "byState = filteredData.groupby(['State']).agg(['mean', 'count'])\n",
        "states = filteredData.State.unique()\n",
        "test = {}\n",
        "for state in states:\n",
        "     test[state] = 0\n",
        "\n",
        "print(\"randomizando datos\\n\")\n",
        "datosRandom = filteredData.sample(frac=1)\n",
        "datosFinales = []\n",
        "print(\"Seleccionando 1000 elementos aleatorios\\n\")\n",
        "for index, row in datosRandom.iterrows():\n",
        "     if test[row['State']] < 1000:\n",
        "             test[row['State']]+=1\n",
        "             datosFinales.append(row)\n",
        "print(\"eliminando estados con menos de 1000 elementos\\n\")\n",
        "for row in datosFinales:\n",
        "     if test[row['State']] < 1000:\n",
        "             datosFinales.remove(row)\n",
        "             print(\"dropping\" + row[\"State\"])\n",
        "\n",
        "exportDataframe = pd.DataFrame(datosFinales)\n",
        "exportDataframe.to_csv('state-samples.csv',  index=False)\n",
        "#medias con los samples cogidos\n",
        "finalStateData = exportDataframe.groupby(['State']).agg(['mean', 'count'])\n",
        "\n",
        "states = filteredData.State.unique()\n",
        "StateIDS = [i++1 for i in range(len(states))]\n",
        "StateIDS.pop()\n",
        "StateIDS.append(0)\n",
        "StateIDS.sort()\n",
        "\n",
        "windDirs = filteredData.Wind_Direction.unique()\n",
        "windDirs.sort()\n",
        "winds = [i++1 for i in range(len(windDirs))]\n",
        "winds.pop()\n",
        "winds.append(0)\n",
        "winds.sort()\n",
        "\n",
        "numeric = exportDataframe\n",
        "numeric = numeric.replace(states, StateIDS)\n",
        "numeric = numeric.replace(windDirs, winds)\n",
        "numeric = numeric.replace([False, True], [0, 1])\n",
        "numeric = numeric.replace(['Night', 'Day'], [0, 1])\n",
        "numeric = numeric.replace(['MapQuest','Bing', 'MapQuest-Bing'], [0,1,2])\n",
        "numeric = numeric.replace(exportDataframe['Timezone'].unique(), [0,1,2,3])\n",
        "numeric = numeric.drop(['County', 'Weather_Condition'], axis=1)\n",
        "numeric.to_csv('numeric-state-samples.csv', index=False)\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejDILxTilh-N",
        "colab_type": "text"
      },
      "source": [
        "#### Dificultades\n",
        "\n",
        "Las principales dificultades a las que nos hemos enfrentado han sido:\n",
        "- La falta de conociemientos a la hora de trabajar con un dataset muy grande\n",
        "- La poca compatibilidad entre las funciones de R y Python para representar y tratar los datos de tipo string.\n",
        "- Interpretación de los datos, dado nuestra poca experiencia con la materia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we09yrTzmPCE",
        "colab_type": "text"
      },
      "source": [
        "# Clasificiación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKyh3sCZmXlQ",
        "colab_type": "text"
      },
      "source": [
        "# Agrupamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFOrHXazmdiR",
        "colab_type": "text"
      },
      "source": [
        "# Conclusiones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf7fxiT5mhfh",
        "colab_type": "text"
      },
      "source": [
        "# Material consultado"
      ]
    }
  ]
}